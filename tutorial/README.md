# LLM Parallelism 101 w/ TorchTitan: Let's scaple up Llama

- We will learn the parallel mechanism behind LLMs with runable code, and train a Llama Model with parallelism technics as building blocks.
- At the end of this tutorial, you will be able to train, fine-tune your own LLMs with Parallelism from scratch.
- We will use TorchTitan and native PyTorch to implement the parallelism, with minimal effort to parallel your own LLM.

## Why Parallelism for LLMs?


## Syllabus
- Chapter 0: Transformer
- Chapter 1: Adding Data Parallelism (DDP, FSDP)
- Chapter 2: Adding Tensor Parallelism (TP)
- Chapter 3: Adding Pipeline Parallelism (PP)
- Chapter 4: Adding ...
